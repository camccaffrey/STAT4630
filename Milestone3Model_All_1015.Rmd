---
title: "Milestone3Model_All.Rmd"
author: "Group 13"
date: "2023-10-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
list.of.packages  =  c("ipred","ROCR","tidyverse", "GGally", "rlang", "ggtext", "viridis", "glue", "countrycode", "MASS", "klaR", "ICS")

# check and install packages
new.packages  =  list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

# load packages
lapply(list.of.packages, require, character.only = TRUE)
```

# a Data Preprocessing 
Since our raw response variable is the "cost of a 1-bedroom apartment in the center of a city", which is a **continuous numeric** variable, our group decides to first transform it into a **binary categorical** variable "expensive".Thus, our group first do a train-test split (70 to 30), and use the median value of "cost of a 1-bedroom apartment in the center of a city" of our training data to dichotomize into 1 and 0 for **both** training and test data.A factor level 1 is corresponding to expensive, and a level 0 suggests inexpensive. We also remove any observation that contains a N/A value. 
```{r}
data = read.csv("Data/cost-of-living_v2.csv")
cost_of_living = data %>% rename(CheapMeal = x1,
                                 meal1 = x2,
                                 meal2 = x2,
                                 mcmeal = x3,
                                 beer.rest.domestic = x4,
                                 beer.rest.imported = x5, 
                                 coffee = x6,
                                 soda = x7,
                                 water.rest = x8,
                                 milk = x9,
                                 bread = x10,
                                 rice = x11,
                                 eggs = x12,
                                 cheese = x13,
                                 chicken = x14,
                                 beef = x15,
                                 apples = x16,
                                 bananas = x17,
                                 oranges = x18,
                                 tomatoes = x19,
                                 potatoes = x20,
                                 onions = x21,
                                 lettuce = x22,
                                 water.market = x23,
                                 wine = x24,
                                 beer.market.domestic = x25,
                                 beer.market.imported = x26,
                                 cigarettes = x27,
                                 ticket = x28,
                                 pass = x29,
                                 taxi.start = x30,
                                 taxi.km = x31,
                                 taxi.hr = x32,
                                 gas = x33,
                                 volkswagen = x34,
                                 toyota = x35,
                                 basic = x36,
                                 mobile = x37,
                                 internet = x38,
                                 gym = x39,
                                 tennis = x40,
                                 cinema = x41,
                                 preschool = x42,
                                 school = x43,
                                 jeans = x44,
                                 dresses = x45,
                                 nikes = x46,
                                 shoes = x47,
                                 rent1.center = x48,
                                 rent1.outer = x49,
                                 rent3.center = x50,
                                 rent3.outer = x51,
                                 sqm.center = x52,
                                 sqm.outer = x53,
                                 salary = x54,
                                 mortgage = x55,
                                 quality = data_quality)
set.seed(4630)
sample.data<-sample.int(nrow(cost_of_living), floor(.7*nrow(cost_of_living)), replace = F) 
train<-cost_of_living[sample.data, ] 
test<-cost_of_living[-sample.data, ]
middle = median(train$rent1.center, na.rm=T)
train<-train%>% mutate(expensive = ifelse(rent1.center>=middle, "1", "0"))
test<- test%>% mutate(expensive = ifelse(rent1.center>=middle, "1", "0"))
train$expensive <- as.factor(train$expensive)
test$expensive <- as.factor(test$expensive)
train <- na.omit(train)
test <- na.omit(test)
```

# b Variable Selection
As our dataset contains 58 predictors, it is not feasible to fit a model with all of these predictors because of the multicollinearity and the high model variance. Thus, based on our EDA, our group selects the most promising predictors to fit our model. We chose predictors based on two criteria: **1) Representative of one group of predictors** and **2) Based on the boxplot, there is a significant different distribution between two categories of our response variable **.
\newline

As a result, our group selects Cheap Meal (x1), Bread Loaf (x10), Eggs (x12), Wine (x24), Gasoline(x33), Utilities (x36), Cinema (x41), and Monthly Salary (x54) to be our preliminary predictors. 
```{r}
train <- train %>%
  rename(
    Expensive=expensive,
    CheapMeal=CheapMeal,
    BreadLoaf=bread,
    Eggs=eggs,
    Wine=wine,
    Gasoline=gas,
    Utilities=basic,
    Cinema=cinema,
    MonthlySalary=salary
  ) %>%
  dplyr::select(
    Expensive,
    CheapMeal,
    BreadLoaf,
    Eggs,
    Wine,
    Gasoline,
    Utilities,
    Cinema,
    MonthlySalary
  )

test <- test %>%
  rename(
    Expensive=expensive,
    CheapMeal=CheapMeal,
    BreadLoaf=bread,
    Eggs=eggs,
    Wine=wine,
    Gasoline=gas,
    Utilities=basic,
    Cinema=cinema,
    MonthlySalary=salary
  ) %>%
  dplyr::select(
    Expensive,
    CheapMeal,
    BreadLoaf,
    Eggs,
    Wine,
    Gasoline,
    Utilities,
    Cinema,
    MonthlySalary
  )
```
By the end of part b, we have our final training data and test data which is well-labeled and ready to be used for Model building. 

# c) Compare LDA and Logistic regression 

## i)
In this subsection, we first fit the logistic regression model and plot the associated ROC curve.Then we fit the LDA model and plot the associated ROC curve.
```{r}
lr_train<-glm(Expensive~., family=binomial, data=train)
summary(lr_train)
lr_preds<-predict(lr_train,newdata=test, type="response")
lr_rates<-ROCR::prediction(lr_preds, test$Expensive)
lr_roc_result<-ROCR::performance(lr_rates,measure="tpr", x.measure="fpr")
plot(lr_roc_result, main="ROC Curve for Logistic Regression")
```

```{r}
lda_train<-MASS::lda(Expensive~., data=train)
lda_preds<-predict(lda_train,test)
lda_rates<-ROCR::prediction(lda_preds$posterior[,2], test$Expensive)
lda_roc_result<-ROCR::performance(lda_rates,measure="tpr", x.measure="fpr")
plot(lda_roc_result, main="ROC Curve for LDA")
```
## ii)
In this subsection, we find the AOC of the previous fitted Logistic Regression and LDA. 
```{r}
lr_auc<-ROCR::performance(lr_rates, measure = "auc")
lr_AUC<-lr_auc@y.values
cat("The AUC of logistic regression is", lr_AUC[[1]], "\n")
lda_auc<-ROCR::performance(lda_rates, measure = "auc")
lda_AUC<-lda_auc@y.values
cat("The AUC of LDA is", lda_AUC[[1]], "\n")
```

## iii)
In real life, a testset isn't readily accessible. Thus, we perform Cross-Validation on the training data as if we don't have the test data. 
```{r}
set.seed(4630)
LR5CV<-boot::cv.glm(train,lr_train, K=5) 
cat("The test error of 5-fold CV of Logistic Regression is", LR5CV$delta[1], "\n")
LR10CV<-boot::cv.glm(train,lr_train, K=10) 
cat("The test error of 10-fold CV of Logistic Regression is", LR10CV$delta[1], "\n")

cv<- function(object, newdata){
return(predict(object, newdata = newdata)$class)
}
set.seed(4630)
LDA5CV<-ipred::errorest(Expensive ~. , data=train, model=MASS::lda,
estimator="cv",
est.para=control.errorest(k=5),
predict=cv)$err
cat("The test error of 5-fold CV of LDA is", LDA5CV, "\n")
LDA10CV<-ipred::errorest(Expensive ~ ., data=train, model=MASS::lda,
estimator="cv",
est.para=control.errorest(k=10),
predict=cv)$err
cat("The test error of 10-fold CV of LDA is", LDA10CV, "\n")

```

## iv)
We report error rate on true test set here. 
```{r}
label<-as.numeric(lr_preds>0.8)
lr_acc_test<-mean(test$Expensive == label)
cat("The test error of logistic regression is", 1-lr_acc_test, "\n")
lda_acc_test<-mean(test$Expensive == lda_preds$class)
cat("The test error of LDA is", 1-lda_acc_test, "\n")
```
We can see that the performance of Logistic Regression and LDA are quite similar. 

# d Improving Logistic Regression Model

## i)

The AUC for our logistic regression model is 0.9526044, showing that our model 
is already much better than random guessing. The test error rate for our model 
is 0.1380208. We can now remove/add predictors to improve these numbers

```{r}
anova(lr_train)

lr_train2 <- glm(Expensive~CheapMeal+BreadLoaf+Eggs+Wine+Utilities+Cinema+MonthlySalary, family = binomial, data = train)
summary(lr_train2)

anova(lr_train, lr_train2)
# remove wine, utilities, cinema, gasoline
```

```{r}
cost_of_living_final2 <- cleaned_data %>%
  rename(
    Expensive=expensive,
    CheapMeal=CheapMeal,
    BreadLoaf=bread,
    Eggs=eggs,
    Wine=wine,
    Gasoline=gas,
    Utilities=basic,
    Cinema=cinema,
    MonthlySalary=salary,
    Coffee = coffee,
    McMeal = mcmeal,
    School = school,
    Ticket = ticket,
    Tennis = tennis,
    Toyota = toyota
    
  ) %>%
  dplyr::select(
    Expensive,
    CheapMeal,
    BreadLoaf,
    Eggs,
    Wine,
    Gasoline,
    Utilities,
    Cinema,
    MonthlySalary,
    Coffee,
    McMeal,
    School,
    Ticket,
    Tennis,
    Toyota
  )


middle = median(cost_of_living_final2$Ticket, na.rm=T)
cost_of_living_final2 = cost_of_living_final2 %>% mutate(Ticket = ifelse(Ticket>=middle, "1", "0"))
cost_of_living_final2$Ticket <- as.factor(cost_of_living_final2$Ticket)
cost_of_living_final2$Ticket
```

Added in Coffee, McMeal, School, Ticket, and Tennis as numerical variables and 
Toyota as a categorical variable(with levels 1> median, 0<median).

Originally had Toyota as numerical variable, but after changing to categorical, 
its p-val changed from  0.000105 to 8.28e-05?

```{r}
set.seed(4630)
sample.data2 <-sample.int(nrow(cost_of_living_final2), floor(.7*nrow(cost_of_living_final2)), replace = F) 
train2<-cost_of_living_final2[sample.data2, ] 
test2<-cost_of_living_final2[-sample.data2, ]
```

```{r}
lr_train3 <-glm(Expensive~ CheapMeal + BreadLoaf + Eggs + MonthlySalary + Coffee + McMeal + School + Ticket + Tennis + Toyota, family = binomial, data = train2)
summary(lr_train3) 
```

```{r}
library(lmtest)

lrtest(lr_train, lr_train3)
lrtest(lr_train2, lr_train3)
```

original vs dropped+added model: p val=1.362e-07. 
significant p-value --> new model fits data better

dropped vs dropped+added model: p-val= 5.388e-07. 
significant p-value --> new model fits data better
